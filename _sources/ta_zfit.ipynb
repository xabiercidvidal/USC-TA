{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Workshop: Advanced Fitting with zfit\n",
    "\n",
    "**Course:** Statistics for Particle Physics  \n",
    "**Duration:** 2-3 Hours  \n",
    "**Level:** Advanced (Closing Session)\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to the final exercise of the course. Over the last 11 hours, we have covered the theory of probabilities, PDFs, Likelihoods, and Confidence Intervals. Now, we will apply this knowledge using **zfit**.\n",
    "\n",
    "### Why zfit?\n",
    "In High Energy Physics (HEP), we often deal with:\n",
    "* **Complex Models:** Sums of signals and backgrounds, convolutions, etc.\n",
    "* **High Statistics:** Millions of events.\n",
    "* **Many Parameters:** Systematics, nuisance parameters.\n",
    "\n",
    "**zfit** is a Python library built on top of **TensorFlow**. It is designed to be:\n",
    "* **Scalable:** Runs on GPUs/CPUs efficiently.\n",
    "* **Pythonic:** Integrates with the Scikit-HEP ecosystem (`uproot`, `mplhep`, `awkward`).\n",
    "* **Flexible:** Allows custom PDFs and arbitrary loss functions.\n",
    "\n",
    "## Agenda\n",
    "1.  **The Basics:** Spaces, Parameters, and PDFs.\n",
    "2.  **The Fit:** Unbinned Likelihoods and Minuit.\n",
    "3.  **Visualization:** Data, Models, and Pull Plots.\n",
    "4.  **Extended Likelihoods:** Fitting Yields.\n",
    "5.  **Constraints:** Handling Systematic Uncertainties.\n",
    "6.  **Simultaneous Fits:** Signal + Control Regions.\n",
    "7.  **2D Fits:** Correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 0. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/jlab/lib/python3.13/site-packages/zfit/__init__.py:93: UserWarning: TensorFlow warnings are by default suppressed by zfit. In order to show them, set the environment variable ZFIT_DISABLE_TF_WARNINGS=0. In order to suppress the TensorFlow warnings AND this warning, set ZFIT_DISABLE_TF_WARNINGS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# need to install zfit mplhep\n",
    "\n",
    "import zfit\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "\n",
    "# Set HEP plotting style (optional but looks nice)\n",
    "plt.style.use(hep.style.ROOT)\n",
    "\n",
    "print(f\"zfit version: {zfit.__version__}\")\n",
    "print(f\"numpy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1. The Building Blocks\n",
    "\n",
    "In zfit, everything is an object. We need to define the \"Universe\" our physics lives in before we can do anything.\n",
    "\n",
    "### 1.1 The Observable Space (`zfit.Space`)\n",
    "The domain of definition for our PDF. If we fit a mass peak, this is the mass range.\n",
    "\n",
    "$$ \\text{Observable} \\in [\\text{min}, \\text{max}] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the B meson mass range\n",
    "obs = zfit.Space(\"mass\", limits=(5000, 6000)) # MeV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Parameters (`zfit.Parameter`)\n",
    "Variables that the fitter can change (float) or that we keep constant (fix).\n",
    "\n",
    "Syntax: `zfit.Parameter(name, value, [lower, upper], [step_size])`\n",
    "\n",
    "* If limits are provided, the parameter is **floating** (fits can change it).\n",
    "* If limits are `None` (or not provided), the parameter is **fixed**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal parameters (Gaussian)\n",
    "mu = zfit.Parameter(\"mu\", 5279, 5200, 5350)     # Floating\n",
    "sigma = zfit.Parameter(\"sigma\", 30, 1, 100)     # Floating\n",
    "\n",
    "# Background parameters (Exponential)\n",
    "lam = zfit.Parameter(\"lambda\", -0.002, -0.01, 0) # Floating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Probability Density Functions (PDFs)\n",
    "zfit provides standard shapes in `zfit.pdf` (Gauss, CrystalBall, Exponential, Voigt, etc.).\n",
    "\n",
    "We will build a simple **Signal + Background** model.\n",
    "\n",
    "$$ PDF_{total}(x) = f_{sig} \\cdot G(x; \\mu, \\sigma) + (1 - f_{sig}) \\cdot E(x; \\lambda) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create component PDFs\n",
    "signal_pdf = zfit.pdf.Gauss(mu=mu, sigma=sigma, obs=obs, name=\"Signal\")\n",
    "bkg_pdf = zfit.pdf.Exponential(lambda_=lam, obs=obs, name=\"Background\")\n",
    "\n",
    "# Create the combined model\n",
    "frac = zfit.Parameter(\"frac\", 0.3, 0, 1) # 30% signal fraction guess\n",
    "model = zfit.pdf.SumPDF([signal_pdf, bkg_pdf], fracs=frac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Generating Data\n",
    "For this exercise, we will generate \"Toy MC\" data from the model itself. In a real analysis, you would load this from a ROOT file or Pandas DataFrame.\n",
    "\n",
    "Note: We can fix the parameters temporarily to generate \"True\" data, then randomize them before fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set \"True\" values for generation\n",
    "mu.set_value(5280)\n",
    "sigma.set_value(25)\n",
    "lam.set_value(-0.003)\n",
    "frac.set_value(0.2)\n",
    "\n",
    "# Sample 5000 events\n",
    "n_events = 5000\n",
    "data = model.sample(n=n_events)\n",
    "\n",
    "# Convert to numpy for basic plotting\n",
    "data_np = data.numpy()\n",
    "print(f\"Generated {len(data_np)} events.\")\n",
    "\n",
    "# Simple check\n",
    "plt.hist(data_np, bins=50, histtype='step');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Fit (Unbinned Likelihood)\n",
    "\n",
    "### 2.1 Theory Recap\n",
    "We use the **Unbinned Negative Log-Likelihood (NLL)**.\n",
    "\n",
    "$$ -\\ln \\mathcal{L}(\\theta) = - \\sum_{i=1}^{N} \\ln P(x_i; \\theta) $$\n",
    "\n",
    "Minimizing this quantity yields the best estimators for parameters $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Randomize parameters so the fit has work to do\n",
    "mu.set_value(5250)\n",
    "sigma.set_value(50)\n",
    "lam.set_value(-0.001)\n",
    "frac.set_value(0.5)\n",
    "\n",
    "# 2. Define the Loss Function\n",
    "nll = zfit.loss.UnbinnedNLL(model=model, data=data)\n",
    "\n",
    "# 3. Define the Minimizer (Minuit)\n",
    "minimizer = zfit.minimize.Minuit()\n",
    "\n",
    "# 4. Run the minimization\n",
    "result = minimizer.minimize(nll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Analyzing the Result\n",
    "The `result` object contains the parameter values, validity checks, and can compute errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Fit Converged: {result.converged}\")\n",
    "print(f\"Fit Valid: {result.valid}\")\n",
    "\n",
    "# Compute Hessian Errors (Parabolic approximation)\n",
    "result.hesse()\n",
    "\n",
    "print(result.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 2.3 Visualization (Best Practices)\n",
    "A fit is worthless if you don't check it visually. In HEP, we almost always plot:\n",
    "1.  **Data** (Error bars)\n",
    "2.  **Model** (Line)\n",
    "3.  **Pulls** (Residuals normalized by error): $\\frac{N_{data} - N_{model}}{\\sigma}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fit_with_pulls(model, data, obs, bins=50):\n",
    "    # Prepare Data\n",
    "    data_np = data.numpy()\n",
    "    lower, upper = obs.limits[0]\n",
    "    \n",
    "    # Histogram the data\n",
    "    counts, edges = np.histogram(data_np, bins=bins, range=(lower, upper))\n",
    "    centers = (edges[:-1] + edges[1:]) / 2\n",
    "    yerr = np.sqrt(counts)\n",
    "\n",
    "    # Evaluate Model\n",
    "    x_plot = np.linspace(lower, upper, 1000)\n",
    "    y_pdf = model.pdf(x_plot).numpy()\n",
    "    # Scale PDF to data count\n",
    "    scale = len(data_np) * (upper - lower) / bins\n",
    "    y_model = y_pdf * scale\n",
    "    \n",
    "    # Evaluate Model at Bin Centers for Pulls\n",
    "    y_model_at_centers = model.pdf(centers).numpy() * scale\n",
    "    pulls = (counts - y_model_at_centers) / yerr\n",
    "\n",
    "    # --- Plotting ---\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 8), sharex=True, gridspec_kw={'height_ratios': [3, 1]})\n",
    "    \n",
    "    # Main Plot\n",
    "    ax1.errorbar(centers, counts, yerr=yerr, fmt='ko', label=\"Data\")\n",
    "    ax1.plot(x_plot, y_model, 'b-', label=\"Total Fit\", linewidth=2)\n",
    "    ax1.set_ylabel(\"Events\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Pull Plot\n",
    "    ax2.errorbar(centers, pulls, yerr=1, fmt='ko')\n",
    "    ax2.axhline(0, color='b', linestyle='--')\n",
    "    ax2.fill_between([lower, upper], -2, 2, color='y', alpha=0.2, label=r\"$2\\sigma$\")\n",
    "    ax2.set_ylabel(\"Pull\")\n",
    "    ax2.set_xlabel(f\"{obs.name} [MeV]\")\n",
    "    ax2.set_ylim(-5, 5)\n",
    "    \n",
    "    plt.subplots_adjust(hspace=0.05)\n",
    "    plt.show()\n",
    "\n",
    "plot_fit_with_pulls(model, data, obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 1: Improving the Model\n",
    "1.  Replace the `zfit.pdf.Gauss` signal with a `zfit.pdf.CrystalBall`.\n",
    "    * You will need two new parameters: `alpha` (try ~1.0) and `n` (try ~2.0).\n",
    "2.  Perform the fit again.\n",
    "3.  Compare the standard deviation (`sigma`) obtained from the Gaussian fit vs the CB fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extended Likelihoods (Fitting Yields)\n",
    "\n",
    "So far, we fitted the **fraction** (`frac`) of signal. The total number of events was fixed to the observed number.\n",
    "\n",
    "In physics, we want to know the **Yield** ($N_{sig}$, $N_{bkg}$) and its uncertainty.\n",
    "We use the **Extended Likelihood**, which adds a Poisson term to the NLL:\n",
    "\n",
    "$$ -\\ln \\mathcal{L}_{ext} = -\\sum \\ln P(x_i) + (N_{exp} - N_{obs} \\ln N_{exp}) $$\n",
    "\n",
    "In zfit, we just attach a yield parameter to the PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Yield Parameters (Floating)\n",
    "n_sig = zfit.Parameter(\"n_sig\", 1000, 0, 10000)\n",
    "n_bkg = zfit.Parameter(\"n_bkg\", 4000, 0, 10000)\n",
    "\n",
    "# 2. Extend the PDFs\n",
    "# Note: We don't need 'frac' anymore. The sum is determined by n_sig + n_bkg\n",
    "signal_ext = signal_pdf.create_extended(n_sig)\n",
    "bkg_ext = bkg_pdf.create_extended(n_bkg)\n",
    "\n",
    "# 3. Create Sum\n",
    "model_ext = zfit.pdf.SumPDF([signal_ext, bkg_ext])\n",
    "\n",
    "# 4. Loss (Note: ExtendedUnbinnedNLL)\n",
    "nll_ext = zfit.loss.ExtendedUnbinnedNLL(model=model_ext, data=data)\n",
    "\n",
    "# 5. Fit\n",
    "result_ext = minimizer.minimize(nll_ext)\n",
    "result_ext.hesse()\n",
    "print(result_ext.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 2: Uncertainty Scaling\n",
    "In a simple counting experiment, the error on $N$ is $\\sqrt{N}$.\n",
    "1.  Check the error on `n_sig` from the fit output.\n",
    "2.  Is it larger or smaller than $\\sqrt{N_{sig}}$?\n",
    "3.  **Thought experiment:** Why is it different? (Hint: Does the background shape look like the signal?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Constraints (Systematics)\n",
    "\n",
    "Often, we know something about a parameter from an external source (e.g., the Jet Energy Scale is known to 1%). We can add this information as a **Constraint**.\n",
    "\n",
    "Mathematically, this multiplies the Likelihood by a Gaussian:\n",
    "$$ \\mathcal{L}_{total} = \\mathcal{L}_{data} \\times G(\\theta; \\mu_{ext}, \\sigma_{ext}) $$\n",
    "\n",
    "Let's assume we have a theoretical prediction for the background slope `lambda` = -0.003 +/- 0.0001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the Constraint\n",
    "# We constraint the parameter 'lam' to -0.003 with width 0.0001\n",
    "constraint = zfit.constraint.GaussianConstraint(params=lam, observation=-0.003, uncertainty=0.0001)\n",
    "\n",
    "# 2. Add to Loss\n",
    "# Constraints are just added to the likelihood\n",
    "nll_constrained = zfit.loss.ExtendedUnbinnedNLL(model=model_ext, data=data, constraints=constraint)\n",
    "\n",
    "# 3. Fit\n",
    "result_c = minimizer.minimize(nll_constrained)\n",
    "result_c.hesse()\n",
    "\n",
    "print(f\"Fitted lambda without constraint: {result_ext.params[lam]['value']:.5f} +/- {result_ext.params[lam]['minuit_hesse']['error']:.5f}\")\n",
    "print(f\"Fitted lambda WITH constraint:    {result_c.params[lam]['value']:.5f} +/- {result_c.params[lam]['minuit_hesse']['error']:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the error on `lambda` decreases, and this might propagate to improve the error on `n_sig`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Simultaneous Fits (Signal + Control Regions)\n",
    "\n",
    "One of the most powerful techniques in HEP. \n",
    "Imagine we have:\n",
    "1.  **Signal Region (SR):** Has Signal and Background.\n",
    "2.  **Control Region (CR):** Has **only** Background (e.g., sideband, or failing a cut).\n",
    "\n",
    "If the background shape is the same in both (e.g. same exponential slope $\\lambda$), we can fit them **simultaneously** to constrain the background much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Setup Data ---\n",
    "# SR Data is 'data' (already generated)\n",
    "\n",
    "# CR Data: Create a new dataset that is PURE background, but shares the same physics (same lambda)\n",
    "# Let's assume CR has 2000 events\n",
    "obs_cr = zfit.Space(\"mass_cr\", limits=(5000, 6000))\n",
    "bkg_cr_gen = zfit.pdf.Exponential(lambda_=-0.003, obs=obs_cr) # Use true lambda\n",
    "data_cr = bkg_cr_gen.sample(2000)\n",
    "\n",
    "# --- 2. Setup Models ---\n",
    "\n",
    "# Shared Parameter (Slope)\n",
    "lambda_shared = zfit.Parameter(\"lambda_shared\", -0.002, -0.01, 0)\n",
    "\n",
    "# SR Model (Sig + Bkg)\n",
    "sig_sr = zfit.pdf.Gauss(mu=mu, sigma=sigma, obs=obs)\n",
    "bkg_sr = zfit.pdf.Exponential(lambda_=lambda_shared, obs=obs) # Uses shared\n",
    "model_sr = zfit.pdf.SumPDF([sig_sr, bkg_sr], fracs=frac)\n",
    "\n",
    "# CR Model (Bkg only)\n",
    "model_cr = zfit.pdf.Exponential(lambda_=lambda_shared, obs=obs_cr) # Uses shared\n",
    "\n",
    "# --- 3. Combined Loss ---\n",
    "nll_sr = zfit.loss.UnbinnedNLL(model_sr, data)\n",
    "nll_cr = zfit.loss.UnbinnedNLL(model_cr, data_cr)\n",
    "\n",
    "simultaneous_nll = nll_sr + nll_cr\n",
    "\n",
    "# --- 4. Fit ---\n",
    "result_sim = minimizer.minimize(simultaneous_nll)\n",
    "print(result_sim.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 3: Shared Resolution\n",
    "Imagine you have a $J/\\psi \\to \\mu\\mu$ calibration channel.\n",
    "1.  Generate a calibration dataset (Pure Gaussian, mean=3100, sigma=25).\n",
    "2.  Set up a simultaneous fit with your Signal Region.\n",
    "3.  Share the `sigma` parameter between the Signal Region and the Calibration Channel.\n",
    "4.  See how this constrains the width of your signal peak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 2D Fits\n",
    "\n",
    "Sometimes, Signal and Background overlap completely in Mass, but separate in another variable (e.g., Decay Time).\n",
    "\n",
    "If variables are uncorrelated: $PDF(x, y) = PDF(x) \\times PDF(y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Spaces\n",
    "mass = zfit.Space(\"mass\", limits=(5000, 6000))\n",
    "time = zfit.Space(\"time\", limits=(0, 2))\n",
    "\n",
    "# 2. Parameters\n",
    "tau_sig = zfit.Parameter(\"tau_sig\", 1.5, 0.1, 3.0)\n",
    "tau_bkg = zfit.Parameter(\"tau_bkg\", 0.4, 0.01, 1.0)\n",
    "\n",
    "# 3. PDFs\n",
    "# Mass\n",
    "sig_m = zfit.pdf.Gauss(mu=mu, sigma=sigma, obs=mass)\n",
    "bkg_m = zfit.pdf.Exponential(lambda_=lam, obs=mass)\n",
    "\n",
    "# Time\n",
    "sig_t = zfit.pdf.Exponential(lambda_=-1/tau_sig, obs=time)\n",
    "bkg_t = zfit.pdf.Exponential(lambda_=-1/tau_bkg, obs=time)\n",
    "\n",
    "# 4. Product PDFs (2D)\n",
    "sig_2d = zfit.pdf.ProductPDF([sig_m, sig_t])\n",
    "bkg_2d = zfit.pdf.ProductPDF([bkg_m, bkg_t])\n",
    "\n",
    "# 5. Total Model\n",
    "model_2d = zfit.pdf.SumPDF([sig_2d, bkg_2d], fracs=0.5)\n",
    "\n",
    "# 6. Generate 2D Data\n",
    "data_2d = model_2d.sample(10000)\n",
    "\n",
    "# 7. Fit\n",
    "nll_2d = zfit.loss.UnbinnedNLL(model_2d, data_2d)\n",
    "result_2d = minimizer.minimize(nll_2d)\n",
    "print(result_2d.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick visualization of 2D data\n",
    "data_np_2d = data_2d.numpy()\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.hist2d(data_np_2d[:,0], data_np_2d[:,1], bins=30, cmap='viridis')\n",
    "plt.xlabel(\"Mass\")\n",
    "plt.ylabel(\"Time\")\n",
    "plt.title(\"2D Data Distribution\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Conclusion & Further Steps\n",
    "\n",
    "In this session, we transitioned from basic Python to full HEP-style fitting.\n",
    "\n",
    "**Summary of Tools:**\n",
    "* `zfit.Space`: Define your physics bounds.\n",
    "* `zfit.Parameter`: Handle what floats and what is fixed.\n",
    "* `ExtendedUnbinnedNLL`: For fitting yields.\n",
    "* `Simultaneous Fit`: Combine SR and CR.\n",
    "\n",
    "**Challenge:**\n",
    "Try to implement a **Conditional PDF** where the width $\\sigma$ depends on the error of the event $\\sigma = \\sigma_{event} \\times S_{scale}$.\n",
    "\n",
    "Good luck with your physics analysis!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
