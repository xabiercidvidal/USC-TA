
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Revisiting the basic concepts &#8212; Statistics for Particle Physics</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'ta_basic_concepts';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Hypothesis testing, simple case" href="ta_hypothesis_test.html" />
    <link rel="prev" title="Introduction" href="intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Statistics for Particle Physics - Home"/>
    <img src="_static/logo.png" class="logo__image only-dark pst-js-only" alt="Statistics for Particle Physics - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Revisiting the basic concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="ta_hypothesis_test.html">Hypothesis testing, simple case</a></li>
<li class="toctree-l1"><a class="reference internal" href="ta_confidence_intervals.html">Confidence Intervals</a></li>
<li class="toctree-l1"><a class="reference internal" href="ta_hypothesis_test_composite.html">Hypothesis testing, composite case</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercises.html">Exercises</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/xabiercidvidal/USC-TA/master?urlpath=tree/docs/ta_basic_concepts.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/xabiercidvidal/USC-TA/blob/master/docs/ta_basic_concepts.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/xabiercidvidal/USC-TA" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/xabiercidvidal/USC-TA/issues/new?title=Issue%20on%20page%20%2Fta_basic_concepts.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/ta_basic_concepts.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Revisiting the basic concepts</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-theorem">Bayes’ Theorem</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kolmogorov-axioms">Kolmogorov axioms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Bayes’ theorem</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-density-functions">Probability density functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-probability-density-functions">Common probability density functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expectation-values">Expectation values</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood">Likelihood</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#frequentist-vs-bayesians">Frequentist vs Bayesians</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood-and-posterior-probability-an-example">Likelihood and posterior  probability - an example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-probability-in-bayesian-statistics">Posterior probability in bayesian statistics</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="revisiting-the-basic-concepts">
<h1>Revisiting the basic concepts<a class="headerlink" href="#revisiting-the-basic-concepts" title="Link to this heading">#</a></h1>
<p><em>Author: Jose A. Hernando, X. Cid Vidal</em>, February 2025</p>
<p><em>Instituto Galego de Altas Enerxías. Universidade de Santiago de Compostela, Spain.</em></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="nb">print</span><span class="p">(</span> <span class="s1">&#39; Last Execution &#39;</span><span class="p">,</span> <span class="n">time</span><span class="o">.</span><span class="n">asctime</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> Last Execution  Wed Feb 14 17:12:08 2024
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># general imports</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="o">%</span><span class="k">reload_ext</span> autoreload
<span class="o">%</span><span class="k">autoreload</span> 2

<span class="c1"># numpy and matplotlib</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span>       <span class="k">as</span> <span class="nn">stats</span>
<span class="kn">import</span> <span class="nn">scipy.optimize</span>     <span class="k">as</span> <span class="nn">optimize</span> 

<span class="kn">import</span> <span class="nn">htintro_examples</span>  <span class="k">as</span> <span class="nn">htexam</span>

<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">context</span><span class="p">(</span><span class="s1">&#39;seaborn-colorblind&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<section id="bayes-theorem">
<h2>Bayes’ Theorem<a class="headerlink" href="#bayes-theorem" title="Link to this heading">#</a></h2>
<section id="kolmogorov-axioms">
<h3>Kolmogorov axioms<a class="headerlink" href="#kolmogorov-axioms" title="Link to this heading">#</a></h3>
<p><strong>Nature is probabilistic.</strong></p>
<p>The starting point of probability are the <strong>Kolmogorov axioms</strong>:</p>
<ul class="simple">
<li><p>The probability for an event, <span class="math notranslate nohighlight">\(E\)</span>, is non negative <span class="math notranslate nohighlight">\(P(E)\ge 0\)</span>.</p></li>
<li><p>The probability for the entire space of possibilities, <span class="math notranslate nohighlight">\(\Omega\)</span>, is one, <span class="math notranslate nohighlight">\(P(\Omega) = 1\)</span>.</p></li>
<li><p>The probability for disjoint events, <span class="math notranslate nohighlight">\(E_1,\dots,E_n\)</span>, is additive, <span class="math notranslate nohighlight">\(P(E_1  \dots  \cup E_n) = \sum_{i=1,n} P(E_i)\)</span>.</p></li>
</ul>
<p>From there we obtain the following corollaries:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(P(A \cup B) = P(A) + P(B) - P(A \cap B) \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(!E) = 1- P(E)\)</span></p>
<p>where <span class="math notranslate nohighlight">\(!E\)</span> is probability that <span class="math notranslate nohighlight">\(E\)</span> does not happen.</p>
</li>
</ul>
<p>A visual example of the Kolmogorov’s axioms and its colloraries:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><a class="reference internal" href="_images/L1_kolmogorov_example.png"><img alt="_images/L1_kolmogorov_example.png" src="_images/L1_kolmogorov_example.png" style="width: 600px;" /></a></p></td>
</tr>
</tbody>
</table>
</div>
<p>which satisfies the following relations:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(A \cup B) = P(A) + P(B) = P(A) + P(!A) = P(\Omega) = 1\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(P(C \cup D) = P(C) + P(D) - P(C \cap D) = P (C) + P(D) - P(E)\)</span>.</p></li>
</ul>
</section>
<section id="id1">
<h3><a class="reference external" href="https://en.wikipedia.org/wiki/Thomas_Bayes">Bayes</a>’ theorem<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p>The probability <span class="math notranslate nohighlight">\(P(A)\)</span> that an event happens is called <strong>marginal probability</strong>.</p>
<p>For example, the probability to roll a dice and get 3.</p>
<p>The probability that an event <span class="math notranslate nohighlight">\(A\)</span> happens if another one <span class="math notranslate nohighlight">\(B\)</span> has happened is called <strong>conditional probability</strong>, <span class="math notranslate nohighlight">\(P(A|B)\)</span>.</p>
<p>For example, the probability that rolling your dice is 3 if you know that the number obtained was odd, in that case <span class="math notranslate nohighlight">\(P(3|\mathrm{odd})=1/3\)</span>.</p>
<p>The Bayes’ theorem states:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(A|B) \, P(B) = P(B|A) \, P(A)\)</span>.</p></li>
</ul>
<p>The conditional probabilities <span class="math notranslate nohighlight">\(P(A|B)\)</span> and <span class="math notranslate nohighlight">\(P(B|A)\)</span> are defined from the probabilities of both events A and B to happen and their marginal probabilities, this is:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(A \cup B) = P(A|B) \, P(B)\)</span> , with <span class="math notranslate nohighlight">\(P(B) \neq 0\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(P(B \cup A) = P(B|A) \, P(A)\)</span> , with <span class="math notranslate nohighlight">\(P(A) \neq 0\)</span>,</p></li>
</ul>
<p>The intersection probabilities between A and B are commutative, <span class="math notranslate nohighlight">\(P(A \cup B) = P(B \cup A)\)</span>, hence, both expressions are equivalent:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(A|B) \, P(B) = P(B|A) \, P(A)\)</span>.</p></li>
</ul>
<p>The <strong>Bayes’ theorem</strong> relates both conditional probabilities: if <span class="math notranslate nohighlight">\(A,B\)</span> are two events with marginal probabilities <span class="math notranslate nohighlight">\(P(A),\, P(B)\)</span>, the conditional probabilities <span class="math notranslate nohighlight">\(P(A|B), \, P(B|A)\)</span> are:</p>
<div class="math notranslate nohighlight">
\[
P(A|B) \, P(B) = P(B|A)\,P(A)
\]</div>
<p><strong>Example:</strong> consider the experiment of rolling two dices, <span class="math notranslate nohighlight">\(A\)</span> is the case when the sum of both dices is 6, and <span class="math notranslate nohighlight">\(B\)</span> when one of them is 4. Verify the Bayes’ theorem.</p>
<p>The probability to get six adding both dices is <span class="math notranslate nohighlight">\(P(A)= 5/36\)</span>, possible outcomes are <span class="math notranslate nohighlight">\(\{(1,5),(2,4),(3,3),(4,2),(5,1)\}\)</span>.</p>
<p>The probability to get four when the addition is six is <span class="math notranslate nohighlight">\(P(B|A)=2/5\)</span>.</p>
<p>The probability to get a dice with four is <span class="math notranslate nohighlight">\(P(B)=11/36\)</span>.</p>
<p>The probability that both add six if one is four is <span class="math notranslate nohighlight">\(2/11\)</span>, therefore:</p>
<div class="math notranslate nohighlight">
\[
\frac{11}{36} \, \frac{2}{11} = \frac{2}{5} \, \frac{5}{36} = \frac{1}{18}
\]</div>
<p><strong>Exercise:</strong> In a low prevalence population, the probability of an individual to have HIV+ is 1 in 1000. Consider a medical test of HIV.</p>
<p>If the patient is HIV+, the test is positive in 99.8 % of the times (true-positive), but if the person is HIV-, the test can wrongly be positive in 0.2 % of the times.</p>
<p>After a test, a patient is diagnosed with HIV+, what is the probability that he has HIV +? Should the doctor repeat the test?</p>
<div class="math notranslate nohighlight">
\[
p(H+ | +) = \frac{p(+ | H+) p(H+)}{p(+)} =  \frac{p(+ | H+) p(H+)}{p(+ | H+) p (H+) + p(+ | H-) p (H-)}
\]</div>
<p>with</p>
<div class="math notranslate nohighlight">
\[
p(H+) = 10^{-3}, \; p(H-) = 1-10^{-3}, \; p(+ | H+) = 0.998, \; p(+ | H-) = 0.002 
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">bayes_pax</span><span class="p">(</span><span class="n">pa</span><span class="p">,</span> <span class="n">pxa</span><span class="p">,</span> <span class="n">pxnoa</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">pxa</span> <span class="o">*</span> <span class="n">pa</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">pxa</span> <span class="o">*</span> <span class="n">pa</span> <span class="o">+</span> <span class="n">pxnoa</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">pa</span><span class="p">))</span>

<span class="n">pax</span> <span class="o">=</span> <span class="n">bayes_pax</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">,</span> <span class="mf">0.998</span><span class="p">,</span> <span class="mf">0.002</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pax</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.3331108144192256
</pre></div>
</div>
</div>
</div>
<p><strong>Exercise:</strong> A neutrino experiment has a problem with the DAQ, and 5% of the running time it does not observe interactions. The neutrinos are produced by a reactor nearby that operates 75% of the time. In this moment, the experiment does not observe neutrinos, what is the probability that the reactor is off?</p>
</section>
</section>
<section id="probability-density-functions">
<h2>Probability density functions<a class="headerlink" href="#probability-density-functions" title="Link to this heading">#</a></h2>
<p>Measurements are of probabilistic nature, there are <strong>random variables</strong> (rv).</p>
<p>The distribution probability, <span class="math notranslate nohighlight">\(g(x)\)</span>, that follows a rv is called <strong>probability density function</strong>, pdf.</p>
<p>If <span class="math notranslate nohighlight">\(x\)</span> takes discrete values, it is called <strong>probability mass function</strong>, pmf.</p>
<p>Here we will abuse the lenguage and call them both as ‘pdf’.</p>
<p>In most of the cases the pdfs depends on some parameters <span class="math notranslate nohighlight">\(\mu\)</span>, indicated as <span class="math notranslate nohighlight">\(g(x | \mu)\)</span>.</p>
<p>From the probability axioms we have:</p>
<div class="math notranslate nohighlight">
\[
\int_{-\infty}^{\infty} g(x) \, \mathrm{d}x = 1
\]</div>
<p>We define the cumulative density function, cdf, <span class="math notranslate nohighlight">\(F(x)\)</span>, as:</p>
<div class="math notranslate nohighlight">
\[
F(x) = \int_{-\infty}^{x} g(x) \, \mathrm{d}x
\]</div>
<section id="common-probability-density-functions">
<h3>Common probability density functions<a class="headerlink" href="#common-probability-density-functions" title="Link to this heading">#</a></h3>
<p>The most common distributions in HEP are:</p>
<ul class="simple">
<li><p><strong>Binomial</strong>. A event can happen with probability <span class="math notranslate nohighlight">\(p\)</span>. The probability to get <span class="math notranslate nohighlight">\(n\)</span> events if we try <span class="math notranslate nohighlight">\(N\)</span> times is given by the binomial pdf.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
g(n|N,p) = \frac{N!}{n! (N-n)!} p^n (1-p)^{N-n}
\]</div>
<ul class="simple">
<li><p><strong>Poisson</strong>. Number of events if we expect <span class="math notranslate nohighlight">\(\nu\)</span>, <em>i.e.</em> the number of interactions in a crossing of the LHC beams is modeled with a poisson. Nuclear decays are poisson distributed poisson too.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
g(n|\nu) = \frac{\nu^n}{n!} e^{-\nu}
\]</div>
<ul class="simple">
<li><p><strong>Uniform</strong>. Equal probability to get <span class="math notranslate nohighlight">\(x\)</span> in an interval <span class="math notranslate nohighlight">\([a, b]\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
g(x|a,b) = \frac{1}{(b-a)}
\]</div>
<ul class="simple">
<li><p><strong>Exponential</strong>. An event can happen in <span class="math notranslate nohighlight">\(x\)</span> with probability <span class="math notranslate nohighlight">\(1/\tau\)</span>. <em>i.e.</em> the decay time of particles.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
g(x|\tau) = \frac{1}{\tau} e^{-x/\tau}
\]</div>
<ul class="simple">
<li><p><strong>Gaussian</strong> (or normal). Associated with the distributions of measurements.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
g(x|\mu,\sigma) = \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
\]</div>
<ul class="simple">
<li><p><strong><span class="math notranslate nohighlight">\(\chi^2_n\)</span></strong> with <span class="math notranslate nohighlight">\(n\)</span> degrees of freedom. Associated with goodness of fit.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
g(x|n) = \frac{1}{2^{n/2} \Gamma(n/2)} x^{n/2-1}e^{-x/2}
\]</div>
<ul class="simple">
<li><p><strong>Breit-Wigner</strong>. Describes the distribution of masses in resonances.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
g(x|\Gamma,x_0) = \frac{1}{\pi} \frac{\Gamma/2}{\Gamma^2/4 + (x-x_0)^2} 
\]</div>
<ul class="simple">
<li><p><strong>Beta</strong>. Associated with the measurement of probabilities. Used in Bayesian statistics.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
g(x | \alpha,\beta) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} x^{\alpha-1} (1-x)^{\beta-1}
\]</div>
<p>where <span class="math notranslate nohighlight">\(x, \alpha, \beta\)</span> are positive.</p>
<ul class="simple">
<li><p><strong>Gamma</strong>. Associated with Bayesian statistics and Poisson distributions.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
g(x | \alpha,\beta) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-x \beta}
\]</div>
<p>where <span class="math notranslate nohighlight">\(x, \alpha, \beta\)</span> are positive.</p>
<p>Some of the pdfs are nicely related:</p>
<p>In the case that an event is rare, has a small probability, <span class="math notranslate nohighlight">\(p\)</span>, but we do a large number of trials, <span class="math notranslate nohighlight">\(N\)</span>, the events distribution follows a binomial with <span class="math notranslate nohighlight">\(g(n | N, p)\)</span>, if <span class="math notranslate nohighlight">\( N \, p = \nu\)</span> is constant, and <span class="math notranslate nohighlight">\(N\)</span> is large enough and <span class="math notranslate nohighlight">\(p\)</span> small enough, the binomial is equivalent to a Poisson with mean <span class="math notranslate nohighlight">\(\nu\)</span>, <span class="math notranslate nohighlight">\(g(n | \nu)\)</span>.</p>
<p>For example nucleus decays follow this rule: large trials (<span class="math notranslate nohighlight">\(N\)</span>, number of nucleus) and small decay probability (<span class="math notranslate nohighlight">\(p\)</span>).</p>
<p>When <span class="math notranslate nohighlight">\(\nu\)</span> is “large”, 12 is enough, the Poisson transforms into Gaussian with <span class="math notranslate nohighlight">\(\mu=\nu, \sigma = \sqrt{\mu}\)</span>.</p>
<p>When we take n values of <span class="math notranslate nohighlight">\(x\)</span>, gaussian distributed, with mean <span class="math notranslate nohighlight">\(\mu\)</span> and sigma <span class="math notranslate nohighlight">\(\sigma\)</span>, and compute <span class="math notranslate nohighlight">\(\chi^2 = \sum_{i=1,n} \frac{(x_i -\mu)^2}{\sigma^2}\)</span>, it follows a chi-squared distribution with <span class="math notranslate nohighlight">\(n\)</span> degrees of freedom.</p>
<p><strong>Exercise:</strong> Compare a poisson distribution, with <span class="math notranslate nohighlight">\(\nu=p \, N\)</span> with a binomial when <span class="math notranslate nohighlight">\(N\)</span> is large and <span class="math notranslate nohighlight">\(p\)</span> is small.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">N</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="mf">0.002</span><span class="p">;</span> <span class="n">ns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">ns</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">ns</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;binomial&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">ns</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">poisson</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">ns</span><span class="p">,</span> <span class="n">N</span><span class="o">*</span><span class="n">p</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span> <span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;poisson&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;n&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;g(n)&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/b40f8e85daed9487e3955db5c2186668ee0507744f89908db0634a12828c3bd6.png" src="_images/b40f8e85daed9487e3955db5c2186668ee0507744f89908db0634a12828c3bd6.png" />
</div>
</div>
<p><strong>Exercise:</strong> Compare a gaussian distribution, with <span class="math notranslate nohighlight">\(\mu=p \, N, \, \sigma = \sqrt{\mu}\)</span> with a binomial when <span class="math notranslate nohighlight">\(N\)</span> is large and <span class="math notranslate nohighlight">\(p\)</span> is small and <span class="math notranslate nohighlight">\(\mu = N p &gt; 50\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">N</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="mf">1e4</span><span class="p">,</span> <span class="mf">50.</span><span class="o">/</span><span class="mf">1e4</span><span class="p">;</span> <span class="n">ns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">ns</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">ns</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>            <span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span> <span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;binomial&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">ns</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">ns</span><span class="p">,</span> <span class="n">N</span><span class="o">*</span><span class="n">p</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">N</span><span class="o">*</span><span class="n">p</span><span class="p">)),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;gaussian&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;g(x)&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/95a2999bdbba937c7ab808cba916f06d5a497c80211ded1924c916b6b8e45aaf.png" src="_images/95a2999bdbba937c7ab808cba916f06d5a497c80211ded1924c916b6b8e45aaf.png" />
</div>
</div>
<p><strong>Exercise:</strong> Check that from a “large” <span class="math notranslate nohighlight">\(\nu\)</span> the poisson distribution is equivalent to a gaussian distribution.</p>
<p><strong>Exercise:</strong> Generate <span class="math notranslate nohighlight">\(m\)</span> datasets, each one with <span class="math notranslate nohighlight">\(n\)</span> data, <span class="math notranslate nohighlight">\(x_i\)</span> distributed random in <span class="math notranslate nohighlight">\([0, 1]\)</span> interval, sum the <span class="math notranslate nohighlight">\(n\)</span> numbers, <span class="math notranslate nohighlight">\(\sum_i^n x_i\)</span> and obtain the distribution of the <span class="math notranslate nohighlight">\(m\)</span> samples.</p>
<p><strong>Exercise:</strong> Generate <span class="math notranslate nohighlight">\(m\)</span> datasets, each with <span class="math notranslate nohighlight">\(n\)</span> data, <span class="math notranslate nohighlight">\(x_i\)</span>, normal distributed, compute its distance squared <span class="math notranslate nohighlight">\(\chi^2 = \sum_{i=1}^n x^2_i\)</span>, what is the distribution of <span class="math notranslate nohighlight">\(\chi^2\)</span>?</p>
<p><strong>Exercise:</strong> Generate <span class="math notranslate nohighlight">\(n\)</span> <span class="math notranslate nohighlight">\(x_i\)</span>-values, each one gaussian distributed with mean <span class="math notranslate nohighlight">\(\mu_i\)</span> and sigma <span class="math notranslate nohighlight">\(\sigma_i\)</span>, show that the sum <span class="math notranslate nohighlight">\(\sum_i x_i\)</span> is gaussian distributed with mean, <span class="math notranslate nohighlight">\(\mu = \sum_i \mu_i\)</span>, and sigma <span class="math notranslate nohighlight">\(\sigma^2 = \sum_i \sigma^2_i\)</span>.</p>
</section>
<section id="expectation-values">
<h3>Expectation values<a class="headerlink" href="#expectation-values" title="Link to this heading">#</a></h3>
<p>Given a <span class="math notranslate nohighlight">\(x\)</span> rv that follows a pdf, <span class="math notranslate nohighlight">\(\, g(x)\)</span>, and a function <span class="math notranslate nohighlight">\(f(x)\)</span> on <span class="math notranslate nohighlight">\(x\)</span>, we define the expected value of <span class="math notranslate nohighlight">\(g(x)\)</span> as:</p>
<div class="math notranslate nohighlight">
\[
E[g(x)] \equiv \int f(x) \, g(x) \, \mathrm{d}x
\]</div>
<p>The <strong>mean</strong>, or average value, is the expected value of <span class="math notranslate nohighlight">\(x\)</span>:</p>
<div class="math notranslate nohighlight">
\[
E[x] = \mu \equiv  \int x \, g(x) \, \mathrm{d}x
\]</div>
<p>The <strong>variance</strong> is the expected value of <span class="math notranslate nohighlight">\((x-\mu)^2\)</span>:</p>
<div class="math notranslate nohighlight">
\[
V[x] = \sigma^2 = E[(x-\mu)^2]= E[x^2]-\mu^2 \equiv \int (x-\mu)^2 \, g(x) \, \mathrm{d}x
\]</div>
<p>We call <strong>standard deviation</strong> to: <span class="math notranslate nohighlight">\(\sigma = \sqrt{V[x]}\)</span></p>
<p>The <strong>expected value</strong> is the <span class="math notranslate nohighlight">\(x\)</span> value with the highest probability <span class="math notranslate nohighlight">\(g(x)\)</span>.</p>
<p>The <strong>median</strong>, is the <span class="math notranslate nohighlight">\(x_{med}\)</span> value that divides the distribution in half,</p>
<div class="math notranslate nohighlight">
\[
\int_0^{x_{med}} g(x) \, \mathrm{d}x = 0.5
\]</div>
<p>For a symmetric pdf, the mean and median are the same.</p>
<p>If <span class="math notranslate nohighlight">\({\bf x}\)</span> is a vector, the pdf is a n-dimensional function.
We define the covariance element between <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> elements as:</p>
<div class="math notranslate nohighlight">
\[
\mathrm{cov}[x_i,x_j] = \int x_i \, x_j \, g({\bf x}) \; \Pi_{i=1,n} \mathrm{d}x_i
\]</div>
<p>If the variables <span class="math notranslate nohighlight">\(x_i, \, x_j\)</span> variables are independent then the covariance is zero.
The contrary is not necessarily true.</p>
<p>We call <strong>marginal</strong> pdf when one or more rvs are integrated out, for example if we integrate <span class="math notranslate nohighlight">\(x_n\)</span>:</p>
<div class="math notranslate nohighlight">
\[
g(x_1,\dots,x_{n-1}) = \int g({\bf x}) \mathrm{d}x_n
\]</div>
<p>We can “project” the pdf in one axis, that is get the marginal pdf for each variable <span class="math notranslate nohighlight">\(x_i\)</span> individually:</p>
<div class="math notranslate nohighlight">
\[
g(x_j) = \int g({\bf x}) \, \Pi_{i=1,n; i \neq j} \, \mathrm{d}x_i
\]</div>
<p><strong>Exercise:</strong> Get the poisson distribution from a Gamma distribution.</p>
<p><strong>Exercise:</strong> Get an exponential distribution from a Gamma distribution.</p>
<p><strong>Exercise:</strong> Get the <span class="math notranslate nohighlight">\(\chi^2\)</span> distribution from a Gamma distribution.</p>
<p><strong>Exercise:</strong> Study the Beta distribution for different parameters of <span class="math notranslate nohighlight">\(\alpha, \beta\)</span> (start <span class="math notranslate nohighlight">\(\alpha=\beta=1\)</span>).</p>
</section>
<section id="likelihood">
<h3>Likelihood<a class="headerlink" href="#likelihood" title="Link to this heading">#</a></h3>
<p>A frequentist evaluate the likelihood of its data.</p>
<p>The <strong>probability density function</strong>, <em>pdf</em>, <span class="math notranslate nohighlight">\(g(x | \mu)\)</span>is the probability to measure <span class="math notranslate nohighlight">\(x\)</span>, which depends on the parameters <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<p>A <strong>likelihood</strong> is the probability evaluted on a observation data, <span class="math notranslate nohighlight">\(x\)</span>, (on data!)</p>
<p>For n, <span class="math notranslate nohighlight">\(x\)</span>, independent measurements that follow a pdf, <span class="math notranslate nohighlight">\(g(x | \mu)\)</span>, the likelihood is the product of the likelihood of each measurement <span class="math notranslate nohighlight">\(x_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(x | \mu) = \Pi_{i=1}^n g(x_i | \mu)
\]</div>
<p>The likelihood can be a very small number, for that reason, it is common to take the (Napierian!) logarithm, and called <strong>log likelihood</strong>:</p>
<div class="math notranslate nohighlight">
\[
\log \mathcal{L}(x |\mu) = \sum_{i=1}^n \log \left(g(x_i |\mu) \right)
\]</div>
<p>If <span class="math notranslate nohighlight">\(\mu\)</span> is known, the likelihood is an real number, if not, <span class="math notranslate nohighlight">\(\mathcal{L}(x | \mu)\)</span> is a function of the <span class="math notranslate nohighlight">\(\mu\)</span> parameter(s).</p>
<p>Given data <span class="math notranslate nohighlight">\(x\)</span>, with <span class="math notranslate nohighlight">\(\mu\)</span> unknown, the likelihood is a function that depends on the parameter(s) <span class="math notranslate nohighlight">\(\mu\)</span>, <span class="math notranslate nohighlight">\(\mathcal{L}(x | \mu)\)</span>.</p>
<p>Frequentists usually estimate the best-parameters, <span class="math notranslate nohighlight">\(\hat{\mu}\)</span>, as the parameters that maximize the likelihood (<span class="math notranslate nohighlight">\(\mathcal{L}(x | \hat{\mu})\)</span> is maximum).</p>
<p>But for convenience, we use instead:</p>
<div class="math notranslate nohighlight">
\[
-2 \log \mathcal{L}(x |\mu)
\]</div>
<p>And then <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> minimize the log-likelihood.</p>
</section>
</section>
<section id="frequentist-vs-bayesians">
<h2>Frequentist vs Bayesians<a class="headerlink" href="#frequentist-vs-bayesians" title="Link to this heading">#</a></h2>
<p>The is a great divide in statistics: frequentists vs Bayesians.</p>
<p><strong>Frequentists are inductive</strong>. <em>They compute the probability of an observation by repeating the same experiment many times</em>. They test is the data is compatible with the theory.</p>
<p><strong>Bayesians are deductive</strong>. <em>They compute the probability using the Bayes’ theorem.
They measure the credibility of a theory based on the data, but they need to assign first a prior probability based in a ‘reasonable’ initial guess</em>.</p>
<p>Frequentists measure a <strong>likelihood function</strong>:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L} (x | \mu)
\]</div>
<p>Frequentists usually estimate the best-parameters, <span class="math notranslate nohighlight">\(\hat{\mu}\)</span>, as the parameters that maximize the likelihood (<span class="math notranslate nohighlight">\(\mathcal{L}(x | \hat{\mu})\)</span> is maximum).</p>
<p>Bayesians measure a <strong>posterior probability</strong>:</p>
<div class="math notranslate nohighlight">
\[
p(\mu | x)
\]</div>
<p>using an initial prior <span class="math notranslate nohighlight">\(\pi(\mu)\)</span> probability and the Bayes’ theorem.</p>
<p>A conversation between a bayesian and a frequentist:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><a class="reference internal" href="_images/bayes_cartoon.png"><img alt="_images/bayes_cartoon.png" src="_images/bayes_cartoon.png" style="width: 300px;" /></a></p></td>
</tr>
</tbody>
</table>
</div>
<p>Most of physicists we are bayesians, but we think of ourselves as frequentists.</p>
<p>Consider the discovery of the Higgs.</p>
<p>A frequentist claims: <em>LHC data strongly disagrees with the hypothesis of the SM without the Higgs and agrees with the existence of the Higgs.</em></p>
<p>While a Bayesian claims: <em>We have discovered the Higgs!</em></p>
<p>A joke in HEP (L. Lyons quotation?):</p>
<p>Frequentists use implecable logic to answer questions that nobody cares about.</p>
<p>Bayesians address the questions that everyone is interested on using assumptions that nobody believes.</p>
<section id="likelihood-and-posterior-probability-an-example">
<h3>Likelihood and posterior  probability - an example<a class="headerlink" href="#likelihood-and-posterior-probability-an-example" title="Link to this heading">#</a></h3>
<p>Let’s consider the basic measurement case seen by a frequentist and by a bayesian.</p>
<p>We just measure a quantity <span class="math notranslate nohighlight">\(\mu\)</span> using a set of data <span class="math notranslate nohighlight">\(x\)</span>. Data <span class="math notranslate nohighlight">\(x\)</span> are gaussian distributed with known sigma, <span class="math notranslate nohighlight">\(\sigma\)</span>. We want to estimate <span class="math notranslate nohighlight">\(\hat{\mu}\)</span>.</p>
<p>For example, <span class="math notranslate nohighlight">\(x\)</span> is a set of values from a gaussian distribution with mean zero and sigma one. How to estimate <span class="math notranslate nohighlight">\(\hat{\mu}\)</span>?</p>
<p>Obvious, it is the average! And the uncertanty in the average is <span class="math notranslate nohighlight">\(\sigma/\sqrt{n}\)</span>, where <span class="math notranslate nohighlight">\(n\)</span> is the number of events in our sample!</p>
<p><strong>Example</strong>: Generate a n-size sample with <span class="math notranslate nohighlight">\(x\)</span> values generated random from a normal gaussian. The likelihood function is drawn as a function of <span class="math notranslate nohighlight">\(\mu\)</span>. The best-estimate <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> is the value where the likelihood is maximum.</p>
<div class="math notranslate nohighlight">
\[
- 2 \log \mathcal{L}({\bf x} | \mu ) = - 2 \sum_{i=1}^n \log g(x_i | \mu)
\]</div>
<p><strong>Explore:</strong> Generate <span class="math notranslate nohighlight">\(n\)</span> samples of a normal. Compute the likelihood as function of the mean (fix sigma of the gaussian to 1).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nsize</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">nsize</span><span class="p">)</span>
<span class="n">htexam</span><span class="o">.</span><span class="n">normal_likelihood</span><span class="p">(</span><span class="n">xs</span><span class="p">);</span>
<span class="c1">#plt.yscale(&#39;log&#39;);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>mu mean : 0.005225524791819061 , mu std : 0.1
</pre></div>
</div>
<img alt="_images/edea826625a83f8cb917a240d559baf682a01484774ad6da0ea720d7a4612f89.png" src="_images/edea826625a83f8cb917a240d559baf682a01484774ad6da0ea720d7a4612f89.png" />
</div>
</div>
</section>
<section id="posterior-probability-in-bayesian-statistics">
<h3>Posterior probability in bayesian statistics<a class="headerlink" href="#posterior-probability-in-bayesian-statistics" title="Link to this heading">#</a></h3>
<p>Bayesians improve a prior probability, “knowledge”, of an hypothesis using data.</p>
<p>Suppose that we have an ensemble of possible hypotheses <span class="math notranslate nohighlight">\(\mu\)</span>, each one with a prior probability <span class="math notranslate nohighlight">\(\pi(\mu)\)</span>.</p>
<p>Of course:</p>
<div class="math notranslate nohighlight">
\[
\int \pi(\mu) \, \mathrm{d}\mu = 1
\]</div>
<p>Or, if the hypotheses are discrete:</p>
<div class="math notranslate nohighlight">
\[
\sum \pi(\mu) = 1
\]</div>
<p>Given some data, <span class="math notranslate nohighlight">\(x\)</span>, Bayes’ theorem allow us to compute the <strong>posterior probability</strong>.</p>
<p>The posterior probability,  <span class="math notranslate nohighlight">\(p(\mu|x)\)</span>, of <span class="math notranslate nohighlight">\(\mu\)</span> given <span class="math notranslate nohighlight">\(x\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
p(\mu|x) = \frac{p(x|\mu) \, \pi(\mu)}{ p(x)}
\]</div>
<p>That is:</p>
<div class="math notranslate nohighlight">
\[
p(\mu|x) = \frac{p(x|\mu) \, \pi(\mu)}{\int p(x|\mu) \pi(\mu) \, \mathrm{d}\mu}
\]</div>
<p>if the hypotheses are discrete:</p>
<div class="math notranslate nohighlight">
\[
p(\mu|x) = \frac{p(x|\mu) \, \pi(\mu)}{\sum p(x|\mu) \pi(\mu)}
\]</div>
<p><strong>Example:</strong> Consider the example above. An sample of <span class="math notranslate nohighlight">\(n\)</span> measurements from a normal distribution. Obtain the posterior probability of the mean, if the sigma is know.</p>
<p>Consider a unique data <span class="math notranslate nohighlight">\(x_0\)</span>, from a gaussian distribution <span class="math notranslate nohighlight">\(g(x | \mu, \sigma = 1)\)</span>, and a ‘reasonable’ uniform prior <span class="math notranslate nohighlight">\(\pi(\mu) = 1\)</span>.</p>
<p>The posterior probability using bayes is:</p>
<div class="math notranslate nohighlight">
\[
p(\mu | x_0) = \frac{g( x_0 | \mu, \sigma = 1) \, \pi(\mu)}{ \int g(x_0 | \mu, \sigma = 1)  \, \pi(\mu) \, \mathrm{d} \mu } = g(x_0 | \mu, \sigma = 1)
\]</div>
<p>that is, a gaussian with sigma unity and centered at <span class="math notranslate nohighlight">\(x_0\)</span>!</p>
<p><strong>Exercise:</strong> Show that the posterior of n measurements, <span class="math notranslate nohighlight">\(x\)</span>, normal distributed, <span class="math notranslate nohighlight">\(g(x | \mu, \sigma)\)</span>, with a flat prior on <span class="math notranslate nohighlight">\(\mu\)</span>, and known <span class="math notranslate nohighlight">\(\sigma\)</span>, is a gaussian with mean <span class="math notranslate nohighlight">\(\mu = \bar{x}\)</span>, the average of the <span class="math notranslate nohighlight">\(n\)</span> <span class="math notranslate nohighlight">\(x\)</span> measurements, and the sigma <span class="math notranslate nohighlight">\(\sigma/\sqrt{n}\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nsize</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">nsize</span><span class="p">)</span>
<span class="c1">#print(xs)</span>
<span class="n">htexam</span><span class="o">.</span><span class="n">normal_posterior</span><span class="p">(</span><span class="n">xs</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>posterior integral 0.995
</pre></div>
</div>
<img alt="_images/9defe610ad2f81089940a033077dbb7d59fe0a29266b586c83c8305d59db0c92.png" src="_images/9defe610ad2f81089940a033077dbb7d59fe0a29266b586c83c8305d59db0c92.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nsize</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">xs</span>    <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">nsize</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span> <span class="n">htexam</span><span class="o">.</span><span class="n">normal_likelihood</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">);</span> <span class="n">htexam</span><span class="o">.</span><span class="n">normal_posterior</span><span class="p">(</span><span class="n">xs</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>mu mean : -0.027511730127815368 , mu std : 0.5
posterior integral 1.000
</pre></div>
</div>
<img alt="_images/01670c2ca306f610fb3fcbc43db21d00fa2349e010d98250e5dc02585f6eb910.png" src="_images/01670c2ca306f610fb3fcbc43db21d00fa2349e010d98250e5dc02585f6eb910.png" />
</div>
</div>
<p><strong>Exercise:</strong> Show that the posterior of a normal prior, <span class="math notranslate nohighlight">\(\pi(\mu | \mu_0, \sigma_{\mu_0})\)</span>,
and a normal likelihood, <span class="math notranslate nohighlight">\(p(x |\mu, \sigma)\)</span>, for n-measurements, <span class="math notranslate nohighlight">\(x\)</span>, is a normal distribution with mean, <span class="math notranslate nohighlight">\(\mu'\)</span>, and sigma, <span class="math notranslate nohighlight">\(\sigma_{\mu'}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{\sigma^2_{\mu'}} = \frac{n}{\sigma^2} + \frac{1}{\sigma^2_{\mu_0}}, 
\;\;\;
\mu' = \sigma^2_{\mu'} \left( \frac{\mu_0}{\sigma^2_{\mu_0}} + \frac{\sum_i x_i}{\sigma^2} \right)
\]</div>
<p><strong>Exercise:</strong> There are several dices in a box, with 4, 6, 12 and 24 sides. We randomly pick one dice and we roll it four times. The outcomes are <span class="math notranslate nohighlight">\(\{1,4,5,2\}\)</span>. What is the posterior probability that the selected dice has 4, 6, 12 or 24 sides? What is the posterior probability if we roll it twice again and wet get now 6 and 1?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">measurements</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">hpriors</span>      <span class="o">=</span> <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">]</span>
<span class="k">for</span> <span class="n">meas</span> <span class="ow">in</span> <span class="n">measurements</span><span class="p">:</span>
    <span class="n">hposteriors</span> <span class="o">=</span> <span class="n">htexam</span><span class="o">.</span><span class="n">dice_posterior</span><span class="p">(</span><span class="n">meas</span><span class="p">,</span> <span class="n">hpriors</span><span class="p">)</span> 
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;data :&#39;</span><span class="p">,</span> <span class="n">meas</span><span class="p">,</span> <span class="s1">&#39; posteriors: &#39;</span><span class="p">,</span> <span class="n">hposteriors</span><span class="p">)</span>
    <span class="n">hpriors</span> <span class="o">=</span> <span class="n">hposteriors</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>!
data : 1  posteriors:  [0.46153846 0.30769231 0.15384615 0.07692308]
!
data : 4  posteriors:  [0.63157895 0.28070175 0.07017544 0.01754386]
!
data : 5  posteriors:  [0.         0.87671233 0.10958904 0.01369863]
!
data : 2  posteriors:  [0.         0.93772894 0.05860806 0.003663  ]
!
data : 6  posteriors:  [0.00000000e+00 9.68779565e-01 3.02743614e-02 9.46073794e-04]
!
data : 1  posteriors:  [0.00000000e+00 9.84378755e-01 1.53809180e-02 2.40326845e-04]
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Introduction</p>
      </div>
    </a>
    <a class="right-next"
       href="ta_hypothesis_test.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Hypothesis testing, simple case</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-theorem">Bayes’ Theorem</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kolmogorov-axioms">Kolmogorov axioms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Bayes’ theorem</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-density-functions">Probability density functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-probability-density-functions">Common probability density functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expectation-values">Expectation values</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood">Likelihood</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#frequentist-vs-bayesians">Frequentist vs Bayesians</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood-and-posterior-probability-an-example">Likelihood and posterior  probability - an example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-probability-in-bayesian-statistics">Posterior probability in bayesian statistics</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By José Ángel Hernando Morata, Xabier Cid Vidal
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright USC - J.A. Hernando, X. Cid Vidal - 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>